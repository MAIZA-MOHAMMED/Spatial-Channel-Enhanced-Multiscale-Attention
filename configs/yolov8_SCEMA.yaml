# YOLO-SCEMA Configuration File
# Paper: Efficient Multiscale Attention with Spatial–Channel Reconstruction for Lightweight Object Detection
# GitHub: https://github.com/MAIZA-MOHAMMED/Spatial-Channel-Enhanced-Multiscale-Attention
# Version: 1.0.0

# ============================================================================
# PROJECT METADATA
# ============================================================================
project:
  name: "YOLO-SCEMA"
  description: "Lightweight Multiscale Attention with Spatial-Channel Reconstruction"
  author: "Mohammed MAIZA"
  version: "1.0.0"
  paper_title: "Efficient Multiscale Attention with Spatial–Channel Reconstruction for Lightweight Object Detection"
  repository: "https://github.com/MAIZA-MOHAMMED/Spatial-Channel-Enhanced-Multiscale-Attention"
  license: "MIT"

# ============================================================================
# MODEL ARCHITECTURE
# ============================================================================
model:
  # Base Architecture
  architecture: "YOLO-SCEMA"
  base_model: "YOLOv8n"
  input_channels: 3
  input_size: [640, 640]  # [height, width]
  
  # Model Size Variants
  size: "n"  # Options: n, s, m, l, x (nano, small, medium, large, xlarge)
  
  # SCEMA Module Configuration
  SCEMA:
    enabled: true
    reduction_ratio: 16
    alpha: 0.5
    compression_ratio: 2
    groups: 4
    kernel_size: 3
    threshold: 0.5
    
  # Backbone Configuration
  backbone:
    type: "CSPDarknet"
    depth_multiple: 0.33  # Scales with model size
    width_multiple: 0.25  # Scales with model size
    
  # Neck Configuration (PAN-FPN)
  neck:
    type: "PAN-FPN"
    depth_multiple: 0.33
    channels: [64, 128, 256]  # For n model
    
  # Head Configuration
  head:
    type: "Anchor-Free"
    num_classes: 80
    strides: [8, 16, 32]
    activation: "SiLU"

# ============================================================================
# TRAINING HYPERPARAMETERS
# ============================================================================
training:
  # Basic Settings
  epochs: 300
  batch_size: 16
  workers: 8
  device: "cuda"  # cuda, cpu, mps
  seed: 42
  
  # Optimizer Configuration
  optimizer:
    name: "AdamW"
    lr: 0.001
    weight_decay: 0.0005
    momentum: 0.937
    nesterov: true
    betas: [0.9, 0.999]
    eps: 1e-8
    
  # Learning Rate Scheduler
  scheduler:
    name: "CosineAnnealingLR"
    warmup_epochs: 3
    warmup_lr: 0.0001
    warmup_momentum: 0.8
    min_lr: 0.000001
    lrf: 0.01  # final lr factor
    
  # Loss Function Configuration
  loss:
    # Component weights
    box_gain: 7.5
    cls_gain: 0.5
    obj_gain: 1.0
    
    # IoU Configuration
    iou_type: "CIoU"
    iou_ratio: 0.05
    
    # Regularization
    label_smoothing: 0.0
    
  # Advanced Training Techniques
  advanced:
    # Gradient Management
    gradient_clip_val: 10.0
    gradient_accumulation_steps: 1
    
    # Mixed Precision
    mixed_precision: true
    amp_level: "O1"
    
    # EMA (Exponential Moving Average)
    ema_enabled: true
    ema_decay: 0.9999
    ema_updates: 100
    
    # Weight Averaging
    weight_averaging: false
    wa_period: 10
    
  # Checkpoint Configuration
  checkpoint:
    save_dir: "./runs/train"
    save_period: 10
    save_best: true
    save_last: true
    max_checkpoints: 10
    
  # Early Stopping
  early_stopping:
    enabled: false
    patience: 50
    min_delta: 0.001
    monitor: "val/mAP50"

# ============================================================================
# DATA AUGMENTATION & PROCESSING
# ============================================================================
data_augmentation:
  # Basic Augmentations
  hsv:
    hue: 0.015
    saturation: 0.7
    value: 0.4
    
  # Geometric Transformations
  geometric:
    rotation: 0.0  # degrees
    translation: 0.2
    scale: 0.9
    shear: 0.0
    perspective: 0.0
    
  # Flip Operations
  flip:
    horizontal: 0.5
    vertical: 0.0
    
  # Advanced Augmentations
  advanced:
    mosaic: 1.0
    mixup: 0.0
    copy_paste: 0.0
    
  # Color Jitter
  color_jitter:
    brightness: 0.2
    contrast: 0.2
    saturation: 0.2
    hue: 0.1
    
  # Other Augmentations
  other:
    motion_blur: 0.2
    median_blur: 0.1
    gaussian_blur: 0.1
    cutout: 0.5

# ============================================================================
# DATASET CONFIGURATION
# ============================================================================
dataset:
  # Dataset Selection
  name: "coco"  # Options: coco, voc, exdark, visdrone, fyp, custom
  path: "./datasets"
  
  # Dataset Splits
  splits:
    train: "train"
    val: "val"
    test: "test"
    
  # Dataset Properties
  properties:
    num_classes: 80
    class_names: &coco_classes [
      "person", "bicycle", "car", "motorcycle", "airplane", "bus", "train", "truck", "boat",
      "traffic light", "fire hydrant", "stop sign", "parking meter", "bench", "bird", "cat",
      "dog", "horse", "sheep", "cow", "elephant", "bear", "zebra", "giraffe", "backpack",
      "umbrella", "handbag", "tie", "suitcase", "frisbee", "skis", "snowboard", "sports ball",
      "kite", "baseball bat", "baseball glove", "skateboard", "surfboard", "tennis racket",
      "bottle", "wine glass", "cup", "fork", "knife", "spoon", "bowl", "banana", "apple",
      "sandwich", "orange", "broccoli", "carrot", "hot dog", "pizza", "donut", "cake",
      "chair", "couch", "potted plant", "bed", "dining table", "toilet", "tv", "laptop",
      "mouse", "remote", "keyboard", "cell phone", "microwave", "oven", "toaster", "sink",
      "refrigerator", "book", "clock", "vase", "scissors", "teddy bear", "hair drier", "toothbrush"
    ]

# ============================================================================
# VALIDATION CONFIGURATION
# ============================================================================
validation:
  # Validation Settings
  frequency: 1  # Validate every N epochs
  batch_size: 32
  
  # Metrics Calculation
  metrics:
    iou_thresholds: [0.5, 0.75, 0.95]
    conf_threshold: 0.001
    nms_threshold: 0.6
    max_detections: 300
    
  # Visualization
  visualization:
    enabled: true
    save_dir: "./runs/val"
    max_images: 100
    conf_threshold: 0.3
    show_labels: true

# ============================================================================
# INFERENCE CONFIGURATION
# ============================================================================
inference:
  # Detection Parameters
  conf_threshold: 0.25
  iou_threshold: 0.45
  max_detections: 300
  
  # Inference Optimization
  optimization:
    multi_scale: false
    augment: false
    half_precision: true  # Use FP16 for inference
    
  # Output Configuration
  output:
    format: "json"  # Options: json, txt, xml, yolo
    save_dir: "./runs/detect"
    save_images: true
    save_labels: true
    save_confidences: true
    
  # Visualization Options
  visualization:
    enabled: true
    thickness: 2
    font_scale: 0.5
    color_palette: "coco"
    show_labels: true
    show_confidences: true

# ============================================================================
# MODEL EXPORT CONFIGURATION
# ============================================================================
export:
  # Export Formats
  formats: ["torchscript", "onnx"]
  
  # ONNX Configuration
  onnx:
    opset_version: 12
    dynamic_axes: true
    simplify: true
    export_params: true
    
  # TensorRT Configuration
  tensorrt:
    enabled: false
    fp16: true
    int8: false
    workspace_size: 4
    max_batch_size: 32
    
  # CoreML Configuration
  coreml:
    enabled: false
    minimum_deployment_target: "iOS13"
    compute_units: "ALL"
    
  # TFLite Configuration
  tflite:
    enabled: false
    quantize: false

# ============================================================================
# LOGGING & MONITORING
# ============================================================================
logging:
  # Log Directory
  log_dir: "./runs"
  experiment_name: "exp"  # Auto-generated if empty
  
  # Loggers Configuration
  loggers:
    tensorboard: true
    csv: true
    json: true
    wandb: false
    mlflow: false
    
  # WandB Configuration (if enabled)
  wandb:
    project: "YOLO-SCEMA"
    entity: ""
    tags: ["object-detection", "lightweight", "attention", "yolo"]
    
  # MLflow Configuration (if enabled)
  mlflow:
    tracking_uri: "http://localhost:5000"
    experiment_name: "YOLO-SCEMA"

# ============================================================================
# EXPERIMENTAL FEATURES
# ============================================================================
experimental:
  # Hyperparameter Optimization
  hyperparameter_optimization:
    enabled: false
    method: "random"  # Options: random, grid, bayesian
    n_trials: 100
    search_space:
      learning_rate:
        min: 0.00001
        max: 0.01
        log: true
      batch_size: [8, 16, 32, 64]
      weight_decay:
        min: 0.000001
        max: 0.001
        log: true
        
  # Knowledge Distillation
  knowledge_distillation:
    enabled: false
    teacher_model: "yolov8x.pt"
    temperature: 3.0
    alpha: 0.5
    
  # Pruning
  pruning:
    enabled: false
    amount: 0.3
    method: "l1_unstructured"

# ============================================================================
# PERFORMANCE PROFILING
# ============================================================================
profiling:
  # Model Profiling
  model_profiling:
    enabled: false
    measure_flops: true
    measure_params: true
    measure_memory: true
    
  # Training Profiling
  training_profiling:
    enabled: false
    profile_batch: 10
    profile_memory: true
    profile_cpu: true
    profile_gpu: true

# ============================================================================
# ENVIRONMENT CONFIGURATION
# ============================================================================
environment:
  # Hardware Configuration
  hardware:
    cuda_visible_devices: "0"
    cpu_threads: 8
    pinned_memory: true
    non_blocking: true
    
  # Distributed Training
  distributed:
    enabled: false
    backend: "nccl"
    init_method: "env://"
    world_size: 1
    rank: 0
    local_rank: 0
    
  # Deterministic Behavior
  deterministic:
    enabled: false
    cudnn_deterministic: true
    cudnn_benchmark: false

# ============================================================================
# MODEL VARIANT SPECIFICATIONS
# ============================================================================
model_variants:
  n:
    name: "nano"
    depth_multiple: 0.33
    width_multiple: 0.25
    channels: [64, 128, 256]
    params: 1.9M
    gflops: 7.4
    description: "Smallest model for edge devices"
    
  s:
    name: "small"
    depth_multiple: 0.33
    width_multiple: 0.50
    channels: [128, 256, 512]
    params: 9.1M
    gflops: 28.6
    description: "Balanced model for general use"
    
  m:
    name: "medium"
    depth_multiple: 0.67
    width_multiple: 0.75
    channels: [192, 384, 768]
    params: 25.0M
    gflops: 78.9
    description: "High accuracy for standard GPUs"
    
  l:
    name: "large"
    depth_multiple: 1.0
    width_multiple: 1.0
    channels: [256, 512, 1024]
    params: 43.7M
    gflops: 165.7
    description: "High performance for server GPUs"
    
  x:
    name: "xlarge"
    depth_multiple: 1.33
    width_multiple: 1.25
    channels: [320, 640, 1280]
    params: 68.2M
    gflops: 257.8
    description: "Maximum accuracy for research"

# ============================================================================
# DATASET SPECIFIC CONFIGURATIONS
# ============================================================================
dataset_configs:
  coco:
    num_classes: 80
    class_names: *coco_classes
    train_split: "train2017"
    val_split: "val2017"
    test_split: "test2017"
    
  voc:
    num_classes: 20
    class_names: &voc_classes [
      "aeroplane", "bicycle", "bird", "boat", "bottle", "bus", "car", "cat", "chair", "cow",
      "diningtable", "dog", "horse", "motorbike", "person", "pottedplant", "sheep", "sofa", "train", "tvmonitor"
    ]
    train_split: "train"
    val_split: "val"
    test_split: "test"
    
  exdark:
    num_classes: 12
    class_names: &exdark_classes [
      "Bicycle", "Boat", "Bottle", "Bus", "Car", "Cat", "Chair", "Cup", "Dog", 
      "Motorbike", "People", "Table"
    ]
    train_split: "train"
    val_split: "val"
    test_split: "test"
    
  visdrone:
    num_classes: 10
    class_names: &visdrone_classes [
      "pedestrian", "people", "bicycle", "car", "van", "truck", "tricycle", 
      "awning-tricycle", "bus", "motor"
    ]
    train_split: "train"
    val_split: "val"
    test_split: "test"
    
  fyp:
    num_classes: 10
    class_names: &fyp_classes [
      "person", "car", "bus", "truck", "motorcycle", "bicycle", 
      "traffic light", "stop sign", "dog", "cat"
    ]
    train_split: "train"
    val_split: "val"
    test_split: "test"

# ============================================================================
# QUICK START CONFIGURATIONS
# ============================================================================
quick_start:
  # Training Quick Start
  training:
    epochs: 100
    batch_size: 32
    learning_rate: 0.01
    model_size: "n"
    
  # Inference Quick Start
  inference:
    conf_threshold: 0.25
    iou_threshold: 0.45
    model_size: "n"
    
  # Export Quick Start
  export:
    format: "onnx"
    img_size: 640
    model_size: "n"

# ============================================================================
# DEFAULT SETTINGS (DO NOT MODIFY)
# ============================================================================
defaults:
  # Model defaults
  model_size: "n"
  input_size: [640, 640]
  
  # Training defaults
  optimizer: "AdamW"
  scheduler: "CosineAnnealingLR"
  
  # Dataset defaults
  dataset_format: "yolo"  # yolo, coco, voc
  
  # Export defaults
  export_format: "onnx"
